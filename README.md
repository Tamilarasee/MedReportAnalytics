# Streamlit Report Analyzer Application

This directory contains the user interface, frontend logic and the generic llm calls for the Medical Report Analyzer application, built using Streamlit.

**❗️ Action Required for Team ❗️**

*   The fine-tuned/local LLM backend (prediction and chat models) is developed and managed separately and **is not included in this repository**.
*   To run this Streamlit application successfully along with finetuned llm, you must have the fine-tuned model's API endpoints running (either locally or deployed).
*   You will need to **modify `app_components.py`** (as detailed in the "Integration" section below) to point the API calls to your specific fine-tuned model endpoints.

## How to Run

1.  Ensure you have the required packages installed (see `requirements.txt`):
    ```bash
    pip install -r requirements.txt 
    ```
2.  Navigate to this directory (`streamlit_app_analyzer`) in your terminal.
3.  Update .env file with API keys
4. Run the application:
    ```bash
    streamlit run streamlit_app.py
    ```

## Purpose

The application allows users to upload medical reports (primarily PDFs), extracts the text content, and provides functionalities for:
- Viewing the extracted text.
- Generating summaries and identifying key medical conditions (prediction).
- Chatting with an AI assistant about the report content.

## Key Files

- `streamlit_app.py`: The main entry point for the Streamlit application. Sets up the basic page structure.
- `app_components.py`: Contains the core UI components and logic for handling file uploads, displaying results, managing chat interactions, and performing analysis. **This is the primary file requiring modification for backend integration.**
- `pdf_extraction.py`: Utility functions for extracting text from PDF files using `pdfplumber`.
- `app_state.py`: Manages the session state for the Streamlit application.
- `requirements.txt`: Lists the necessary Python packages for this application.
- `Dataset/`: Placeholder or contains sample data for testing.

## Integration with Fine-Tuned LLM

Currently, the analysis and chat functionalities might be using generic LLMs (like local Ollama or standard OpenAI models selected via `load_llm_model`) or are placeholders. To leverage the specialized fine-tuned medical LLM (presumably served from a separate backend, potentially the `Finetuned_model` directory), the following areas within `app_components.py` need to be updated:

1.  **Medical Condition Prediction/Analysis (within `analyze_view()` function):**
    - Locate the section inside `analyze_view` (starts around line 178) where the `summary_prompt` (line ~197) and `diagnosis_prompt` (line ~205) are defined and used to generate results with the loaded `llm` object.
    - **Modify this part:** Instead of invoking the generic `llm` (e.g., `ChatOpenAI`, `ChatOllama`), insert code to make an API call (e.g., using `requests`) to your fine-tuned model's prediction/analysis endpoint. Send the `st.session_state.report_text` and appropriate instructions derived from the prompts. Parse the API response to populate `st.session_state.summary` and `st.session_state.diagnosis_data`.

2.  **Report Chat Functionality (within `chat_view()` function):**
    - Find the section inside `chat_view` (starts around line 441) where the user's input (`prompt`) is taken and a response is generated by invoking the loaded `llm` object.
    - **Modify this part:** Replace the call to the generic `llm` with an API call (e.g., using `requests`) to your fine-tuned model's chat endpoint. Send the necessary context, including chat history (`st.session_state.messages`), the report text (`st.session_state.report_text`), and the user's `prompt`. Use the API response as the AI's reply in the chat interface.

These integrations will likely involve adding LLM calls (alongside `ChatOllama` or `ChatOpenAI` instances obtained via `load_llm_model`) with `requests` calls to the specific API endpoints serving the fine-tuned model. 